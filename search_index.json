[["index.html", "Python 网络爬虫 Chapter 1 前言 1.1 课程大纲 1.2 ", " Python 网络爬虫 Jim Zhang 2022-11-26 Chapter 1 前言 1.1 课程大纲 1.2 "],["网络爬虫简介.html", "Chapter 2 网络爬虫简介", " Chapter 2 网络爬虫简介 我们一般所说的网络爬虫通俗而言应该叫网页抓取、网页数据提取。 很容易理解，“抓取”这个词包括两个字，一个是“抓”，一个是“取”，抓就是从互联网上获取网页，取就是从获取的网页中提取我们需要的数据。 抓，即从互联网上获取网页，这个过程就是网络爬虫的核心，也是网络爬虫的难点。这一部分的程序需要透过 HTTP 协议，向网站服务器发送请求，获取网页数据。 取，这一部分相对简单一点，因为这只是对数据的解析、搜索和重新格式化，并将数据复制到电子表格或加载到数据库中。 "],["http-协议简介.html", "Chapter 3 HTTP 协议简介 3.1 Web 客户端与服务器 3.2 Web 资源 3.3 MIME 类型 3.4 URI 和 URL 3.5 事务", " Chapter 3 HTTP 协议简介 本章参考书为 D. Gourley et al. 2002. HTTP: The Definitive Guide。 Web 浏览器、服务器和其他的相关软件都是基于 HTTP 协议通讯的。可以说，HTTP 是现代全球互联网中使用的通用语言（注意，它是一种协议而不是语言），它是 Web 的基础。 3.1 Web 客户端与服务器 Web 内容都是存储在 Web 服务器上的。这些服务器存储了互联网中的数据，如果客户端发出请求的话，他们会提供数据。对于 HTTP 协议而言，客户端向服务器发送 HTTP 请求，服务器返回 HTTP 响应。客户端与服务器（client-server）一同构成了万维网的基本组件。 最典型的 Web 客户端是 Web 浏览器，它们可以向 Web 服务器发送请求，然后接收并通过渲染显示服务器返回的数据。 3.2 Web 资源 Web 服务器上的每个对象都被称为 Web 资源（Web resource）。最简单的 Web 资源就是服务器上的静态文件。这些文件包含任意内容：文本文件、Adobe 的 Acrobat PDF 文件、图像文件、视频文件等等。但是资源也可以是脚本程序，比如 JavaScript、TypeScript 等脚本，这些脚本可以帮你搜索股票价格、在地图上显示你的位置、或者是其他任何你想要的功能。 3.3 MIME 类型 互联网上有数千种不同的数据类型，HTTP 仔细地给每种要通过 Web 传输的对象都打上了名为 MIME 类型（Multipurpose Internet Mail Extensions，多用途互联网邮件扩展）的标签。最开始 MIME 类型是用来标记电子邮件的，但是现在它已经被广泛地用于 Web 上的所有数据类型。 Web 服务器会为所有的 HTTP 对象数据附加一个 MIME 类型，如图所示。当 Web 客户端（尤其是浏览器）接收到一个 HTTP 响应时，它会根据 MIME 类型来决定如何处理响应的数据。 Source: Servercake India MIME 类型是一个字符串，它的格式为 type/subtype，比如 text/html、image/jpeg、application/pdf 等等。MIME 类型的第一个部分是主类型（type），第二个部分是子类型（subtype）。主类型用来描述数据的大类，子类型用来描述数据的具体类型。比如，text/html 就是 HTML 文档，text/plain 就是纯文本文档，image/jpeg 就是 JPEG 图像，image/gif 就是 GIF 图像，application/pdf 就是 PDF 文档。 常见的 MIME 类型有数百个，实验性或用途有限的 MIME 类型则更多。 3.4 URI 和 URL URI（Uniform Resource Identifier，统一资源标识符）是一个用来标识互联网上资源的字符串。URI 有两种形式：URL（Uniform Resource Locator，统一资源定位符）和 URN（Uniform Resource Name，统一资源名称）。URL 用来定位资源，URN 用来命名资源。 URL 用来定位资源，它的格式为 scheme://host:port/path，比如 http://www.example.com:80/index.html。URL 的第一部分是方案（scheme），它用来描述如何访问资源。比如，http、https、ftp、file 等等。URL 的第二部分是主机（host），它用来描述资源所在的服务器。URL 的第三部分是端口（port），它用来描述服务器上的应用程序所监听的端口。URL 的第四部分是路径（path），它用来描述资源在服务器上的位置。比如，/index.html、/images/logo.png 等等。 URN 用来命名资源，它的格式为 urn:namespace:identifier，比如 urn:isbn:978-7-121-15535-2。URN 的第一部分是命名空间（namespace），它用来描述资源的命名空间。URN 的第二部分是标识符（identifier），它用来描述资源的标识符。 3.5 事务 HTTP 事务是一个 HTTP 请求和一个 HTTP 响应的组合。这种通信是通过名为 HTTP 报文的格式化数据块进行的。HTTP 事务的格式如下： Request-Line headers CRLF body Status-Line headers CRLF body 3.5.1 方法 在 HTTP 协议中，会有几种不同的请求命令，这些命令被称为方法（method）。HTTP 方法用来描述请求的动作类型。HTTP 方法有很多，常见的有 GET、POST、PUT、DELETE、HEAD、OPTIONS、TRACE 等等。 方法 描述 GET 请求获取 Request-URI 所标识的资源。 POST 在 Request-URI 所标识的资源后附加新的数据。 PUT 请求服务器存储一个资源，并用 Request-URI 作为其标识。 DELETE 请求服务器删除 Request-URI 所标识的资源。 HEAD 请求获取由 Request-URI 所标识的资源的响应消息报头。 OPTIONS 请求查询服务器的性能，或者查询与资源相关的选项和需求。 TRACE 请求服务器回送收到的请求信息，主要用于测试或诊断。 3.5.2 状态码 每条 HTTP 响应报文都会包含一个状态码，它用来描述请求的处理结果。HTTP 状态码有很多，常见的有 200、301、302、304、400、401、403、404、500、502、503 等等。 状态码 描述 解释 200 OK 请求成功，请求所希望的响应头或数据体将随此响应返回。 301 Moved Permanently 请求的资源已被分配了新的 URI，以后应使用资源现在所指的 URI。 302 Found 请求的资源临时从不同的 URI 响应请求。 304 Not Modified 自从上次请求后，请求的资源未修改过。 400 Bad Request 服务器无法理解请求的格式，客户端不应当尝试再次使用相同的内容发起请求。 401 Unauthorized 请求未授权。这个状态代码必须和 WWW-Authenticate 报头域一起使用。 403 Forbidden 服务器收到请求，但是拒绝提供服务。 404 Not Found 请求资源不存在，eg：输入了错误的 URL。 500 Internal Server Error 最常见的服务器端错误。 502 Bad Gateway 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503 Service Unavailable 由于临时的服务器维护或者过载，服务器当前无法处理请求。 "],["html-简介.html", "Chapter 4 HTML 简介", " Chapter 4 HTML 简介 超文本标记语言（英语：HyperText Markup Language，简称：HTML）是一种用于创建网页的标准标记语言。HTML 是一种基础技术，常与 CSS、JavaScript 一起被众多网站用于设计网页、网页应用程序以及移动应用程序的用户界面。网页浏览器可以读取 HTML 文件，并将其渲染成可视化网页。HTML 描述了一个网站的结构语义随着线索的呈现，使之成为一种标记语言而非编程语言。 HTML 元素是构建网站的基石。HTML 允许嵌入图像与对象，并且可以用于创建交互式表单，它被用来结构化信息——例如标题、段落和列表等等，也可用来在一定程度上描述文档的外观和语义。HTML 的语言形式为尖括号包围的 HTML 元素（如 &lt;html&gt;），浏览器使用 HTML 标签和脚本来诠释网页内容，但不会将它们显示在页面上。 举个简单的例子，下面的 HTML 代码创建了一个标题和一个段落： &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Example&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Example&lt;/h1&gt; &lt;p&gt;This is an example.&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 其结构如下： "],["beautifulsoup-教程.html", "Chapter 5 BeautifulSoup 教程 5.1 安装 5.2 BeautifulSoup", " Chapter 5 BeautifulSoup 教程 5.1 安装 BeautifulSoup 得名于《爱丽丝梦游仙境》第十章： Beautiful Soup, so rich and green, Waiting in a hot tureen! Who for such dainties would not stoop? Soup of the evening, beautiful Soup! Soup of the evening, beautiful Soup! … Chapter 10, Alice’s Adventures in Wonderland, Lewis Carroll. 意为“化腐朽为神奇”。BeautifulSoup 和 Selenium 的安装很方便，只需要一行命令： $ pip install BeautifulSoup4, selenium 请注意，不是 pip install BeautifulSoup，而是 pip install BeautifulSoup4，因为前者指的是 BeautifulSoup3。 安装完成后，我们可以通过 python 交互式控制台来检验是否成功安装，比如： Python 3.10.7 (tags/v3.10.7:6cc6b13, Sep 5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)] on win32 Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. &gt;&gt;&gt; from bs4 import BeautifulSoup, import selenium &gt;&gt;&gt; 如果没有报错信息，就代表安装成功啦。 5.2 BeautifulSoup 5.2.1 本地 html 文件 首先，我们可以先用一个本地 html 文件做测试，这个文件内容如下： &lt;!--test.html--&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;这是测试。&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Test&lt;/h1&gt; &lt;p&gt;这是一个测试。&lt;/p&gt; &lt;p&gt;这是第二个测试。&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 它对应的 DOM（Document Object Model，文档对象模型）树如下： Document └── html ├── head │ └── title └── body ├── h1 ├── p └── p 这个树形结构为我们讲述了如下信息： Document 是整个文档的根节点，它的子节点是 html。 html 是文档的根元素，它的子节点是 head、body 等。 head 是文档的头部，它的子节点是 title 等。 body 是文档的主体，它的子节点是 h1、p 等。 title、h1、p 等都是文档的叶子节点，它们没有子节点。 然后，我们可以通过 BeautifulSoup 来读取这个文件，代码如下： from bs4 import BeautifulSoup with open(&#39;test.html&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;) as f: soup = BeautifulSoup(f, &#39;html.parser&#39;) print(soup) 输出结果如下： &lt;html&gt; &lt;head&gt; &lt;title&gt;这是测试。&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Test&lt;/h1&gt; &lt;p&gt;这是一个测试。&lt;/p&gt; &lt;p&gt;这是第二个测试。&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 这样，我们就可以通过 soup 来访问这个 html 文件了。进一步地，我们可以通过 soup 来访问它的子节点，比如： print(soup.html.head.title) 输出结果如下： &lt;title&gt;这是测试。&lt;/title&gt; 我们可以看到，soup.html.head.title 就是 title 节点，对于整个文档唯一的节点（title 就是这样），我们可以通过 soup.title 来直接访问它。但是对于非唯一的节点就比较麻烦，比如第一个 p 节点，我们可以通过 soup.body.p 来访问它，但是如果我们想要访问第二个 p 节点，就没办法了。这时候，我们可以通过 find_all 方法来访问它们，比如： print(soup.find_all(&#39;p&#39;)) 输出结果如下： [&lt;p&gt;这是一个测试。&lt;/p&gt;, &lt;p&gt;这是第二个测试。&lt;/p&gt;] 获取标签中的内容，对每个 tag 使用 text 属性即可，比如： print(soup.title.text) 输出结果如下： 这是测试。 5.2.2 网页 接下来，我们来看看如何使用 BeautifulSoup 来爬取网页。首先，为了利用python 帮我们完成发送请求的操作，我们需要导入 requests 库（安装方法我们就不重复介绍了），然后通过 requests 来获取网页的内容。 现在有这样一个网页：https://q-weather.info/weather/54399/history/?date=2022-09-16，其内容大致如下 存储了海淀区的天气信息。现在想获取这个网页数据，我们如下操作： import requests, pandas as pd from bs4 import BeautifulSoup url = &#39;https://q-weather.info/weather/54399/history/&#39; params = {&#39;date&#39;: &#39;2022-09-16&#39;} r = requests.get(url, params=params) soup = BeautifulSoup(r.text, &#39;html.parser&#39;) 这样这个 soup 就存储这个网页的内容了。接下来的任务显而易见：首先获取表格的标题行，然后是数据单元格，最后是把他们存储起来。 table = soup.table columns = [th.text for th in table.thead.find_all(&#39;th&#39;)] data = [[td.text for td in tr.find_all(&#39;td&#39;)] for tr in table.tbody.find_all(&#39;tr&#39;)] df = pd.DataFrame(data, columns=columns) print(df.head) 输出结果如下： 时次 瞬时温度 地面气压 相对湿度 瞬时风向 瞬时风速 1小时极大风速 1小时降水 10分钟平均能见度 0 2022-09-16 01:00 +0800 20.1 1002.5 87 0.0 0.4 0.0 9.3 1 2022-09-16 02:00 +0800 19.5 1002.1 91 45/NE 0.5 0.5 0.0 5.4 2 2022-09-16 03:00 +0800 19.2 1001.9 91 290/WNW 1.4 1.6 0.0 8.1 3 2022-09-16 04:00 +0800 19.6 1001.7 92 25/NNE 0.3 2.1 0.0 4.9 4 2022-09-16 05:00 +0800 18.9 1001.7 93 28/NNE 0.4 1.0 0.0 4.0 这就是 BeautifulSoup 的基本用法了，更多的用法可以参考官方文档：https://www.crummy.com/software/BeautifulSoup/bs4/doc/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
